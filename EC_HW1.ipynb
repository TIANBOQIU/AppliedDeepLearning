{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EC-HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TIANBOQIU/AppliedDeepLearning/blob/master/EC_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zcZDgVVShRvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "59e6e9fd-b40a-430c-98c5-83cfa1329452"
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190206)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190206)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "voWo3znvhZuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.nn import relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6fP1UOhhviu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## EC1 ##\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Types are needed later when calculating loss\n",
        "# using the ```sparse_softmax_cross_entropy_with_logits``` we chose to \n",
        "# compare against.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBwcDI8iqXFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import math_ops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOb0sDKPh25g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = relu(x)\n",
        "    x = self.d2(x)\n",
        "    return x \n",
        "  \n",
        "model = MyModel()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def built_in_loss(logits, labels):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "\n",
        "# Mean Square Error\n",
        "def our_loss(logits, labels, n_classes=10):\n",
        "  labels = tf.one_hot(labels, n_classes, dtype=tf.float32)\n",
        "  return K.mean(math_ops.square(labels-logits), axis=-1)\n",
        "\n",
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Forward pass\n",
        "    logits = model(images)\n",
        "    loss_one = built_in_loss(logits, labels)\n",
        "    loss_two = our_loss(logits, labels)    \n",
        "    \n",
        "  # Backward pass\n",
        "  # I'll use our implementation to update the gradients.\n",
        "  grads = tape.gradient(loss_two, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_one, loss_two\n",
        "\n",
        "def calc_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  acc = tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
        "  return acc * 100\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDpgJsSZmrWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1646
        },
        "outputId": "9afded58-481b-42e4-8a3a-fbe9e568d625"
      },
      "cell_type": "code",
      "source": [
        "# Loop over the dataset, grab batchs, and train our model\n",
        "# As we go, verify the loss returned by our implementation is\n",
        "# the same as the built-in methods.\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Epoch\", epoch + 1, \"\\n\")\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss_one, loss_two = train_on_batch(model, images, labels)\n",
        "    \n",
        "    # You can use something like this as a quick sanity check\n",
        "    #tf.debugging.assert_near(loss_one, loss_two, atol=0.001, rtol=0.001)\n",
        "    #print(\"loss1\",loss_one.shape, K.mean(loss_one))\n",
        "    #print(\"loss2\",loss_two.shape, K.mean(loss_two))\n",
        "    step = optimizer.iterations.numpy() \n",
        "    if step % 100 == 0:\n",
        "      print(\"Step\", step)\n",
        "      print(\"Built-in loss: {} MSE: {}\".format(K.mean(loss_one),K.mean(loss_two)))\n",
        "      print(\"\")\n",
        "      \n",
        "  print('Train accuracy %.2f' % calc_accuracy(model(x_train), y_train))\n",
        "  print('Test accuracy %.2f\\n' % calc_accuracy(model(x_test), y_test))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "\n",
            "Step 200\n",
            "Built-in loss: 1.6683100461959839 MSE: 0.023214392364025116\n",
            "\n",
            "Step 300\n",
            "Built-in loss: 1.6642494201660156 MSE: 0.02293778955936432\n",
            "\n",
            "Step 400\n",
            "Built-in loss: 1.643017292022705 MSE: 0.01974768191576004\n",
            "\n",
            "Step 500\n",
            "Built-in loss: 1.6161428689956665 MSE: 0.01742279902100563\n",
            "\n",
            "Train accuracy 94.98\n",
            "Test accuracy 94.87\n",
            "\n",
            "Epoch 2 \n",
            "\n",
            "Step 600\n",
            "Built-in loss: 1.610154390335083 MSE: 0.015985924750566483\n",
            "\n",
            "Step 700\n",
            "Built-in loss: 1.5701311826705933 MSE: 0.014738786965608597\n",
            "\n",
            "Step 800\n",
            "Built-in loss: 1.5795440673828125 MSE: 0.014546427875757217\n",
            "\n",
            "Step 900\n",
            "Built-in loss: 1.5902549028396606 MSE: 0.014589463360607624\n",
            "\n",
            "Step 1000\n",
            "Built-in loss: 1.590810775756836 MSE: 0.0164126455783844\n",
            "\n",
            "Train accuracy 96.28\n",
            "Test accuracy 95.85\n",
            "\n",
            "Epoch 3 \n",
            "\n",
            "Step 1100\n",
            "Built-in loss: 1.5799155235290527 MSE: 0.01332764606922865\n",
            "\n",
            "Step 1200\n",
            "Built-in loss: 1.5705146789550781 MSE: 0.011924189515411854\n",
            "\n",
            "Step 1300\n",
            "Built-in loss: 1.5632209777832031 MSE: 0.01144436001777649\n",
            "\n",
            "Step 1400\n",
            "Built-in loss: 1.6118595600128174 MSE: 0.014635728672146797\n",
            "\n",
            "Step 1500\n",
            "Built-in loss: 1.5687329769134521 MSE: 0.010500079952180386\n",
            "\n",
            "Train accuracy 96.89\n",
            "Test accuracy 96.41\n",
            "\n",
            "Epoch 4 \n",
            "\n",
            "Step 1600\n",
            "Built-in loss: 1.5623396635055542 MSE: 0.012085705995559692\n",
            "\n",
            "Step 1700\n",
            "Built-in loss: 1.566910743713379 MSE: 0.009663664735853672\n",
            "\n",
            "Step 1800\n",
            "Built-in loss: 1.5579874515533447 MSE: 0.009401807561516762\n",
            "\n",
            "Step 1900\n",
            "Built-in loss: 1.548109531402588 MSE: 0.011032863520085812\n",
            "\n",
            "Train accuracy 97.33\n",
            "Test accuracy 96.78\n",
            "\n",
            "Epoch 5 \n",
            "\n",
            "Step 2000\n",
            "Built-in loss: 1.5409975051879883 MSE: 0.00853226613253355\n",
            "\n",
            "Step 2100\n",
            "Built-in loss: 1.5642935037612915 MSE: 0.00956985354423523\n",
            "\n",
            "Step 2200\n",
            "Built-in loss: 1.5322258472442627 MSE: 0.00804470106959343\n",
            "\n",
            "Step 2300\n",
            "Built-in loss: 1.5406649112701416 MSE: 0.009025676175951958\n",
            "\n",
            "Step 2400\n",
            "Built-in loss: 1.5577036142349243 MSE: 0.009429829195141792\n",
            "\n",
            "Train accuracy 97.61\n",
            "Test accuracy 96.75\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SBeEzEf1va5C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When the mean spuare loss decreases over iterations, the cross entropy loss decreases very slow. Using MSE is like to treat the process as a regression problem rather than a classification problem."
      ]
    },
    {
      "metadata": {
        "id": "BuLYIG-MyY2f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#EC2\n",
        "\n",
        "build custom a Dense layer"
      ]
    },
    {
      "metadata": {
        "id": "RDT-XIBUoP99",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.nn import relu\n",
        "\n",
        "\n",
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_outputs):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "    self.num_outputs = num_outputs\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_variable(\"kernel\", shape=[int(input_shape[-1]),\n",
        "                                                    self.num_outputs])\n",
        "  def call(self, input):\n",
        "    return tf.matmul(input, self.kernel)\n",
        "\n",
        "\n",
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = MyDenseLayer(128)\n",
        "    self.d2 = MyDenseLayer(10)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = relu(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "  \n",
        "def loss_function(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))\n",
        "\n",
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # forward pass\n",
        "    logits = model(images)\n",
        "    loss = loss_function(logits, labels)\n",
        "    # backward pass\n",
        "    grads = tape.gradient(loss, model.variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.variables))\n",
        "    return loss # loss.shape == (batch_size, )\n",
        "\n",
        "def acc(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
        "  return accuracy * 100\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZBV9CVy5G1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Types are needed later when calculating loss\n",
        "# using the ```sparse_softmax_cross_entropy_with_logits``` we chose to \n",
        "# compare against.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV-H3KmF5TVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "63739642-769c-4c04-a9be-65206b3590b4"
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "\n",
        "model = MyModel()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Epoch {}: \".format(epoch+1))\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss = train_on_batch(model, images, labels)\n",
        "    step = optimizer.iterations.numpy()\n",
        "    if step % 100 == 0:\n",
        "      print(\"\\tstep {}, loss: {}\".format(step, loss))\n",
        " "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: \n",
            "\tstep 100, loss: 0.29075515270233154\n",
            "\tstep 200, loss: 0.3028695583343506\n",
            "\tstep 300, loss: 0.2816504240036011\n",
            "\tstep 400, loss: 0.20934338867664337\n",
            "Epoch 2: \n",
            "\tstep 500, loss: 0.17968139052391052\n",
            "\tstep 600, loss: 0.1882302314043045\n",
            "\tstep 700, loss: 0.2294468879699707\n",
            "\tstep 800, loss: 0.17239172756671906\n",
            "\tstep 900, loss: 0.09766191989183426\n",
            "Epoch 3: \n",
            "\tstep 1000, loss: 0.07699155807495117\n",
            "\tstep 1100, loss: 0.061417948454618454\n",
            "\tstep 1200, loss: 0.08709728717803955\n",
            "\tstep 1300, loss: 0.07212940603494644\n",
            "\tstep 1400, loss: 0.11893992125988007\n",
            "Epoch 4: \n",
            "\tstep 1500, loss: 0.09180206060409546\n",
            "\tstep 1600, loss: 0.12146177887916565\n",
            "\tstep 1700, loss: 0.08273693919181824\n",
            "\tstep 1800, loss: 0.1405399888753891\n",
            "Epoch 5: \n",
            "\tstep 1900, loss: 0.1556229293346405\n",
            "\tstep 2000, loss: 0.06317076832056046\n",
            "\tstep 2100, loss: 0.0340818352997303\n",
            "\tstep 2200, loss: 0.034171249717473984\n",
            "\tstep 2300, loss: 0.06998111307621002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lM4TbXDm_IT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The accuracy of the model with custom Dense layers:**"
      ]
    },
    {
      "metadata": {
        "id": "A74HUjZf7NEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "61145784-6c99-4536-f562-7f3e9f0efebb"
      },
      "cell_type": "code",
      "source": [
        "acc_train = acc(model(x_train), y_train).numpy()\n",
        "acc_test = acc(model(x_test), y_test).numpy()\n",
        "print(\"Training accuracy {:.2f}%\".format(acc_train))\n",
        "print(\"Test accuracy {:.2f}%\".format(acc_test))\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy 98.25%\n",
            "Test accuracy 97.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jvdv_1FLAG1y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Compare with the built-in Dense layer:**"
      ]
    },
    {
      "metadata": {
        "id": "h-5z0UF_AFcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}